{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms.functional as TF\n\nimport pytorch_lightning as pl\nfrom typing import Dict, List, Tuple","metadata":{"execution":{"iopub.status.busy":"2023-05-23T08:54:55.772626Z","iopub.execute_input":"2023-05-23T08:54:55.773049Z","iopub.status.idle":"2023-05-23T08:54:55.779368Z","shell.execute_reply.started":"2023-05-23T08:54:55.773015Z","shell.execute_reply":"2023-05-23T08:54:55.777983Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class DoubleConv(pl.LightningModule):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            \n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False), # bias=False for Batchnorm              \n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            \n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv(x)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T08:53:40.300188Z","iopub.execute_input":"2023-05-23T08:53:40.300487Z","iopub.status.idle":"2023-05-23T08:53:40.308034Z","shell.execute_reply.started":"2023-05-23T08:53:40.300461Z","shell.execute_reply":"2023-05-23T08:53:40.306949Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class UNET(pl.LightningModule):\n    def __init__(\n        self, \n        in_channels, \n        out_channels, \n        optimizer, \n        loss_fn,\n        features=[64, 128, 256, 512],\n    ):\n        super(UNET, self).__init__()\n        \n        self.loss_fn = loss_fn\n        self.optimizer = loss_fn\n        \n        self.train_dice_score = 0\n        self.val_dice_score = 0\n\n        \n        # architechture          \n        self.ups = nn.ModuleList()\n        self.downs = nn.ModuleList()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Down part of UNET - only left side \n        for feature in features:\n            self.downs.append(DoubleConv(in_channels, feature))\n            in_channels = feature\n\n        # Up part of UNET - only right side \n        for feature in reversed(features):\n            # up             \n            self.ups.append(\n                nn.ConvTranspose2d(\n                    # with skip-conn\n                    feature*2, feature, kernel_size=2, stride=2,\n                )\n            )\n            # two times right, x gets concat to 2xchannel         \n            self.ups.append(DoubleConv(feature*2, feature))\n\n        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n\n        \n    def forward(self, x):\n        skip_connections = []\n\n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n\n        x = self.bottleneck(x)\n        \n        # reverse skip_connections \n        skip_connections = skip_connections[::-1]\n            \n                        # пропускаем DoubleConv        \n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)\n            skip_connection = skip_connections[idx//2]\n\n            # resize if x does not match skip_conn              \n            if x.shape != skip_connection.shape:\n                x = TF.resize(x, size=skip_connection.shape[2:], antialias=True)\n\n            concat_skip = torch.cat((skip_connection, x), dim=1)\n            x = self.ups[idx+1](concat_skip)\n\n        return self.final_conv(x)\n    \n    \n    def configure_optimizers(self):\n        return self.optimizer\n    \n    \n    def training_step(self, batch: torch.Tensor, batch_idx) -> Dict[str, torch.Tensor]:\n        x, y = batch\n        pred = self(x.float())\n        loss = self.loss_fn(pred, y)\n        pred = torch.sigmoid(pred)\n        pred = (pred > 0.5).float()\n        \n        self.train_dice_score += (2 * (pred * y).sum()) / ((pred + y).sum() + 1e-8)\n        self.log(\"train_loss\", loss, prog_bar=True)\n        return {\"loss\": loss}\n\n    def training_epoch_end(self, output: List[Dict[str, torch.Tensor]]):\n        dice_score = self.train_dice_score / len(output)\n        self.log(\"train_dice_score\", dice_score, prog_bar=True)\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> Dict[str, torch.Tensor]:\n        x, y = batch\n        pred = self(x.float())\n        loss = self.loss_fn(pred, y)\n        pred = torch.sigmoid(pred)\n        pred = (pred > 0.5).float()\n\n        self.val_dice_score += (2 * (pred * y).sum()) / ((pred + y).sum() + 1e-8)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        return {\"loss\": loss}\n\n    def validation_epoch_end(self, output: List[Dict[str, torch.Tensor]]):\n        dice_score = self.val_dice_score / len(output)\n        self.log(\"val_dice_score\", dice_score, prog_bar=True)","metadata":{"_uuid":"390ba9fa-21c3-4c9e-92bd-2a086c7667fd","_cell_guid":"58317a6c-dd82-44e4-ada8-a57d2c24db80","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-23T08:55:00.930869Z","iopub.execute_input":"2023-05-23T08:55:00.931267Z","iopub.status.idle":"2023-05-23T08:55:00.957261Z","shell.execute_reply.started":"2023-05-23T08:55:00.931236Z","shell.execute_reply":"2023-05-23T08:55:00.955808Z"},"trusted":true},"execution_count":8,"outputs":[]}]}